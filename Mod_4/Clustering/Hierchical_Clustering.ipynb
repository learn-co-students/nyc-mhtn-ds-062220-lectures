{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierchical Clustering \n",
    "Agenda today:\n",
    "- Hierarchical Clustering \n",
    "    -  Introduction\n",
    "    -  How the algorithm is constructed?\n",
    "    -  Implementing hierarchical algorithm\n",
    "\n",
    "Goals today:\n",
    "- Understand how hierarchical clustering finds clusterings in observations\n",
    "- Compare and contrast hierchical clustering and k-means\n",
    "- Build and interpret a dendrogram\n",
    "***\n",
    "# Introduction \n",
    "\n",
    "### Supervised vs. Unsupervised Machine Learning \n",
    "Recall from our previous lesson in supervised learning, where we use a number of features to predict a label. In unsupervised ML problem, we are not predicting and labels, thus we do not have a ground truth to compare our models to. In other words, we are doing the best we can to group individual observations together without having a gold standard to evaluate them. \n",
    "\n",
    "### Intuition\n",
    "Clustering is a family of techniques for identifying clusters in a dataset. The goal of clustering is to group the most similar observations together into a cluster. In Hierchical Clustering, we prioritize **similarity** between individual observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering\n",
    "Recall K-means clustering where the goal is to assign individual observations to pre-specified number of clusters according to Euclidean distance between the centroid and the observation, Hierarchical clustering sets out to group the most similar two observations together from a bottom-up level. We end up with a tree-like diagram named **dendrogram**, which allows us to view the clusterings obtained for each possible number of clusters, from 1 to n. It is up to our discretion as data scientists to decide how many clusters we want. \n",
    "***\n",
    "One disadvantage of K-means clustering is that we have to specify the number of clusters beforehand. The type of hierchical clustering we will learn today is **agglomerative**, or **bottom up**, such that we do not have to specify the number of clusters beforehand. We will now dive into the details of hierchical clustering.\n",
    "***\n",
    "### How does the algorithm work\n",
    "Initially, every observation is its own cluster. As we move up the leaf of the dendrogram, the most similar pair of observation fuse together, and the next most similar group of leafs fuse together etc. until everything fuse together into a big cluster. Where to cut off branching that fuse together the tree is up to our discretion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from plot_hci import plot_agglomerative_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of hierarchical aggplomerative clustering \n",
    "- Single Linkage \n",
    "    -  Minimum pair-wise distance: for any two clusters, take one observation from each and determine their minimum distance. Do this over and over, until you have identified the overall minimum pair-wise distance. \n",
    "- Complete Linkage\n",
    "    -  Nearest may be defined as the furthest (or maximum) distance between two clusters. That is, all possible pairwise distances between elements (one from cluster A and one from B) are evaluated and the largest value is used as the distance between clusters A & B. This is sometimes called complete linkage and is also called furthest neighbor.\n",
    "- Average Linkage\n",
    "    - The distance between clusters is defined as the average distance between the average values of each of the data points in the clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,3,10,11,19,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets generate some data and look at an example of hierarchical agglomerative clustering \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(1000)  \n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[50,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X = np.concatenate((a, b),)\n",
    "print(X.shape)  # 150 samples with 2 dimensions\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dendrogram in scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(X, 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#c, coph_dists = cophenet(Z, pdist(X))\n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate and construct the dendrogram \n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[61],X[85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trimming and truncating the dendrogram \n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=4,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# from documentation of \"lastp\"\n",
    "# The last p non-singleton formed in the linkage are the only non-leaf nodes in the linkage; \n",
    "# they correspond to rows Z[n-p-2:end] in Z. All other non-singleton clusters are contracted into leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the scikitlearn module hierarchical clustering to perform the same task \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "np.random.seed(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "m = 16\n",
    "X, y = make_blobs(n_samples= m, n_features=2, centers=k, cluster_std=1.3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
    "agg_clust\n",
    "assigned_clust = agg_clust.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = assigned_clust, s = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try clustering on the iris dataset \n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "# in this case, we won't be working with predicting labels, so we will only use the features (X)\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_iris[:,0],X_iris[:,2]) #c = y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "iris_cluster\n",
    "pred_iris_clust = iris_cluster.fit_predict(X_iris)\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 2], c = pred_iris_clust, s = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare it to the actual truth \n",
    "plt.scatter(X_iris[:,0],X_iris[:,2], c = y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluation - silhouette score \n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_iris, pred_iris_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating number of clusters / Cut points\n",
    "For hierarchical agglomerative clustering, or clustering in general, it is generally difficult to truly evaluate the results. Therefore, it is up you, the data scientists, to decide.\n",
    "\n",
    "When we are viewing dendrograms for hierarchical agglomerative, we can visually examine where the natural cutoff is, despite it not sounding exactly statistical, or scientific. We might want to interpret the clusters and assign meanings to them depending on domain-specific knowledge and shape of dendrogram. However, we can evaluate the quality of our clusters using measurements such as Sihouette score discussed in the k-means lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of hierarchical clustering\n",
    "\n",
    "#### Advantages\n",
    "- Intuitive and easy to implement\n",
    "- More informative than k-means because it takes individual relationship into consideration\n",
    "- Allows us to look at dendrogram and decide number of clusters\n",
    "\n",
    "#### Disadvantages\n",
    "- Very sensitive to outliers\n",
    "- Cannot undo the previous merge, which might lead to problems later on \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
