{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation with Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "#SK LEARN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will euse the Kings County housing dataset in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/learn-co-curriculum/dsc-mod-2-project-v2-1/master/kc_house_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df['yr_sold']=  df['date'].map(lambda x: x.year)\n",
    "\n",
    "df['yrs_old'] =  df['yr_built'].map(lambda x: 2016-x)\n",
    "\n",
    "df['yr_since_reno'] =  df['yr_renovated'].map(lambda x: 2016-x if x > 0 else np.nan)\n",
    "\n",
    "df['yrs_since_update'] = df.apply(lambda x: min(x['yrs_old'], x['yr_since_reno']), axis=1)\n",
    "\n",
    "df['bedrooms']=df['bedrooms'].map(lambda x: x if x < 10 else 10)\n",
    "\n",
    "df.replace('?', 0, inplace=True)\n",
    "\n",
    "df['sqft_basement'] = pd.to_numeric(df['sqft_basement'])\n",
    "\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_df = pd.get_dummies(df['zipcode'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= df.price\n",
    "\n",
    "features = df.drop(columns=['date', 'price', 'lat', 'long', 'yr_built', 'yr_renovated', 'yr_since_reno', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features_2= PolynomialFeatures(degree=2, include_bias=False)\n",
    "features_poly = polynomial_features_2.fit_transform(features)\n",
    "poly_columns = polynomial_features_2.get_feature_names(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_poly = pd.DataFrame(features_poly, columns=poly_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_poly = pd.merge(features_poly, zip_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>yr_sold</th>\n",
       "      <th>yrs_old</th>\n",
       "      <th>yrs_since_update</th>\n",
       "      <th>bedrooms^2</th>\n",
       "      <th>bedrooms bathrooms</th>\n",
       "      <th>bedrooms sqft_living</th>\n",
       "      <th>bedrooms sqft_lot</th>\n",
       "      <th>bedrooms floors</th>\n",
       "      <th>bedrooms waterfront</th>\n",
       "      <th>bedrooms view</th>\n",
       "      <th>bedrooms condition</th>\n",
       "      <th>bedrooms grade</th>\n",
       "      <th>bedrooms sqft_above</th>\n",
       "      <th>bedrooms sqft_basement</th>\n",
       "      <th>bedrooms sqft_living15</th>\n",
       "      <th>bedrooms sqft_lot15</th>\n",
       "      <th>bedrooms yr_sold</th>\n",
       "      <th>bedrooms yrs_old</th>\n",
       "      <th>bedrooms yrs_since_update</th>\n",
       "      <th>bathrooms^2</th>\n",
       "      <th>bathrooms sqft_living</th>\n",
       "      <th>bathrooms sqft_lot</th>\n",
       "      <th>bathrooms floors</th>\n",
       "      <th>bathrooms waterfront</th>\n",
       "      <th>bathrooms view</th>\n",
       "      <th>bathrooms condition</th>\n",
       "      <th>bathrooms grade</th>\n",
       "      <th>bathrooms sqft_above</th>\n",
       "      <th>bathrooms sqft_basement</th>\n",
       "      <th>bathrooms sqft_living15</th>\n",
       "      <th>bathrooms sqft_lot15</th>\n",
       "      <th>bathrooms yr_sold</th>\n",
       "      <th>bathrooms yrs_old</th>\n",
       "      <th>bathrooms yrs_since_update</th>\n",
       "      <th>sqft_living^2</th>\n",
       "      <th>sqft_living sqft_lot</th>\n",
       "      <th>sqft_living floors</th>\n",
       "      <th>...</th>\n",
       "      <th>98032</th>\n",
       "      <th>98033</th>\n",
       "      <th>98034</th>\n",
       "      <th>98038</th>\n",
       "      <th>98039</th>\n",
       "      <th>98040</th>\n",
       "      <th>98042</th>\n",
       "      <th>98045</th>\n",
       "      <th>98052</th>\n",
       "      <th>98053</th>\n",
       "      <th>98055</th>\n",
       "      <th>98056</th>\n",
       "      <th>98058</th>\n",
       "      <th>98059</th>\n",
       "      <th>98065</th>\n",
       "      <th>98070</th>\n",
       "      <th>98072</th>\n",
       "      <th>98074</th>\n",
       "      <th>98075</th>\n",
       "      <th>98077</th>\n",
       "      <th>98092</th>\n",
       "      <th>98102</th>\n",
       "      <th>98103</th>\n",
       "      <th>98105</th>\n",
       "      <th>98106</th>\n",
       "      <th>98107</th>\n",
       "      <th>98108</th>\n",
       "      <th>98109</th>\n",
       "      <th>98112</th>\n",
       "      <th>98115</th>\n",
       "      <th>98116</th>\n",
       "      <th>98117</th>\n",
       "      <th>98118</th>\n",
       "      <th>98119</th>\n",
       "      <th>98122</th>\n",
       "      <th>98125</th>\n",
       "      <th>98126</th>\n",
       "      <th>98133</th>\n",
       "      <th>98136</th>\n",
       "      <th>98144</th>\n",
       "      <th>98146</th>\n",
       "      <th>98148</th>\n",
       "      <th>98155</th>\n",
       "      <th>98166</th>\n",
       "      <th>98168</th>\n",
       "      <th>98177</th>\n",
       "      <th>98178</th>\n",
       "      <th>98188</th>\n",
       "      <th>98198</th>\n",
       "      <th>98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3540.0</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4020.0</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>6042.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.00</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>1392400.0</td>\n",
       "      <td>6667000.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>7639.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>7710.0</td>\n",
       "      <td>21726.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6510.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>5070.0</td>\n",
       "      <td>22917.0</td>\n",
       "      <td>6042.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0625</td>\n",
       "      <td>5782.5</td>\n",
       "      <td>16294.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>15.75</td>\n",
       "      <td>4882.5</td>\n",
       "      <td>900.0</td>\n",
       "      <td>3802.5</td>\n",
       "      <td>17187.75</td>\n",
       "      <td>4531.5</td>\n",
       "      <td>146.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>6604900.0</td>\n",
       "      <td>18611940.0</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5440.0</td>\n",
       "      <td>16124.0</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.00</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>83.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>592900.0</td>\n",
       "      <td>7700000.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>7840.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>5440.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>8056.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>4080.0</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>6042.0</td>\n",
       "      <td>153.00</td>\n",
       "      <td>153.00</td>\n",
       "      <td>3841600.0</td>\n",
       "      <td>9800000.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>24240.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>22509.0</td>\n",
       "      <td>6045.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>16160.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>15006.00</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>58.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>2822400.0</td>\n",
       "      <td>13574400.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  view  \\\n",
       "0       3.0       1.00       1180.0    5650.0     1.0         0.0   0.0   \n",
       "1       3.0       2.25       2570.0    7242.0     2.0         0.0   0.0   \n",
       "2       2.0       1.00        770.0   10000.0     1.0         0.0   0.0   \n",
       "3       4.0       3.00       1960.0    5000.0     1.0         0.0   0.0   \n",
       "4       3.0       2.00       1680.0    8080.0     1.0         0.0   0.0   \n",
       "\n",
       "   condition  grade  sqft_above  sqft_basement  sqft_living15  sqft_lot15  \\\n",
       "0        3.0    7.0      1180.0            0.0         1340.0      5650.0   \n",
       "1        3.0    7.0      2170.0          400.0         1690.0      7639.0   \n",
       "2        3.0    6.0       770.0            0.0         2720.0      8062.0   \n",
       "3        5.0    7.0      1050.0          910.0         1360.0      5000.0   \n",
       "4        3.0    8.0      1680.0            0.0         1800.0      7503.0   \n",
       "\n",
       "   yr_sold  yrs_old  yrs_since_update  bedrooms^2  bedrooms bathrooms  \\\n",
       "0   2014.0     61.0              61.0         9.0                3.00   \n",
       "1   2014.0     65.0              25.0         9.0                6.75   \n",
       "2   2015.0     83.0              83.0         4.0                2.00   \n",
       "3   2014.0     51.0              51.0        16.0               12.00   \n",
       "4   2015.0     29.0              29.0         9.0                6.00   \n",
       "\n",
       "   bedrooms sqft_living  bedrooms sqft_lot  bedrooms floors  \\\n",
       "0                3540.0            16950.0              3.0   \n",
       "1                7710.0            21726.0              6.0   \n",
       "2                1540.0            20000.0              2.0   \n",
       "3                7840.0            20000.0              4.0   \n",
       "4                5040.0            24240.0              3.0   \n",
       "\n",
       "   bedrooms waterfront  bedrooms view  bedrooms condition  bedrooms grade  \\\n",
       "0                  0.0            0.0                 9.0            21.0   \n",
       "1                  0.0            0.0                 9.0            21.0   \n",
       "2                  0.0            0.0                 6.0            12.0   \n",
       "3                  0.0            0.0                20.0            28.0   \n",
       "4                  0.0            0.0                 9.0            24.0   \n",
       "\n",
       "   bedrooms sqft_above  bedrooms sqft_basement  bedrooms sqft_living15  \\\n",
       "0               3540.0                     0.0                  4020.0   \n",
       "1               6510.0                  1200.0                  5070.0   \n",
       "2               1540.0                     0.0                  5440.0   \n",
       "3               4200.0                  3640.0                  5440.0   \n",
       "4               5040.0                     0.0                  5400.0   \n",
       "\n",
       "   bedrooms sqft_lot15  bedrooms yr_sold  bedrooms yrs_old  \\\n",
       "0              16950.0            6042.0             183.0   \n",
       "1              22917.0            6042.0             195.0   \n",
       "2              16124.0            4030.0             166.0   \n",
       "3              20000.0            8056.0             204.0   \n",
       "4              22509.0            6045.0              87.0   \n",
       "\n",
       "   bedrooms yrs_since_update  bathrooms^2  bathrooms sqft_living  \\\n",
       "0                      183.0       1.0000                 1180.0   \n",
       "1                       75.0       5.0625                 5782.5   \n",
       "2                      166.0       1.0000                  770.0   \n",
       "3                      204.0       9.0000                 5880.0   \n",
       "4                       87.0       4.0000                 3360.0   \n",
       "\n",
       "   bathrooms sqft_lot  bathrooms floors  bathrooms waterfront  bathrooms view  \\\n",
       "0              5650.0               1.0                   0.0             0.0   \n",
       "1             16294.5               4.5                   0.0             0.0   \n",
       "2             10000.0               1.0                   0.0             0.0   \n",
       "3             15000.0               3.0                   0.0             0.0   \n",
       "4             16160.0               2.0                   0.0             0.0   \n",
       "\n",
       "   bathrooms condition  bathrooms grade  bathrooms sqft_above  \\\n",
       "0                 3.00             7.00                1180.0   \n",
       "1                 6.75            15.75                4882.5   \n",
       "2                 3.00             6.00                 770.0   \n",
       "3                15.00            21.00                3150.0   \n",
       "4                 6.00            16.00                3360.0   \n",
       "\n",
       "   bathrooms sqft_basement  bathrooms sqft_living15  bathrooms sqft_lot15  \\\n",
       "0                      0.0                   1340.0               5650.00   \n",
       "1                    900.0                   3802.5              17187.75   \n",
       "2                      0.0                   2720.0               8062.00   \n",
       "3                   2730.0                   4080.0              15000.00   \n",
       "4                      0.0                   3600.0              15006.00   \n",
       "\n",
       "   bathrooms yr_sold  bathrooms yrs_old  bathrooms yrs_since_update  \\\n",
       "0             2014.0              61.00                       61.00   \n",
       "1             4531.5             146.25                       56.25   \n",
       "2             2015.0              83.00                       83.00   \n",
       "3             6042.0             153.00                      153.00   \n",
       "4             4030.0              58.00                       58.00   \n",
       "\n",
       "   sqft_living^2  sqft_living sqft_lot  sqft_living floors  ...  98032  98033  \\\n",
       "0      1392400.0             6667000.0              1180.0  ...      0      0   \n",
       "1      6604900.0            18611940.0              5140.0  ...      0      0   \n",
       "2       592900.0             7700000.0               770.0  ...      0      0   \n",
       "3      3841600.0             9800000.0              1960.0  ...      0      0   \n",
       "4      2822400.0            13574400.0              1680.0  ...      0      0   \n",
       "\n",
       "   98034  98038  98039  98040  98042  98045  98052  98053  98055  98056  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0   \n",
       "1      0      0      0      0      0      0      0      0      0      0   \n",
       "2      0      0      0      0      0      0      0      0      0      0   \n",
       "3      0      0      0      0      0      0      0      0      0      0   \n",
       "4      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   98058  98059  98065  98070  98072  98074  98075  98077  98092  98102  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0   \n",
       "1      0      0      0      0      0      0      0      0      0      0   \n",
       "2      0      0      0      0      0      0      0      0      0      0   \n",
       "3      0      0      0      0      0      0      0      0      0      0   \n",
       "4      0      0      0      0      0      1      0      0      0      0   \n",
       "\n",
       "   98103  98105  98106  98107  98108  98109  98112  98115  98116  98117  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0   \n",
       "1      0      0      0      0      0      0      0      0      0      0   \n",
       "2      0      0      0      0      0      0      0      0      0      0   \n",
       "3      0      0      0      0      0      0      0      0      0      0   \n",
       "4      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   98118  98119  98122  98125  98126  98133  98136  98144  98146  98148  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0   \n",
       "1      0      0      0      1      0      0      0      0      0      0   \n",
       "2      0      0      0      0      0      0      0      0      0      0   \n",
       "3      0      0      0      0      0      0      1      0      0      0   \n",
       "4      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   98155  98166  98168  98177  98178  98188  98198  98199  \n",
       "0      0      0      0      0      1      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 221 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call train_test_split on the data and capture the results\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_poly, target, random_state=22,test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled =pd.DataFrame(data=scaler.transform(X_train), columns=features_poly.columns)\n",
    "X_test_scaled =pd.DataFrame(data=scaler.transform(X_test), columns=features_poly.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>yr_sold</th>\n",
       "      <th>yrs_old</th>\n",
       "      <th>yrs_since_update</th>\n",
       "      <th>bedrooms^2</th>\n",
       "      <th>bedrooms bathrooms</th>\n",
       "      <th>bedrooms sqft_living</th>\n",
       "      <th>bedrooms sqft_lot</th>\n",
       "      <th>bedrooms floors</th>\n",
       "      <th>bedrooms waterfront</th>\n",
       "      <th>bedrooms view</th>\n",
       "      <th>bedrooms condition</th>\n",
       "      <th>bedrooms grade</th>\n",
       "      <th>bedrooms sqft_above</th>\n",
       "      <th>bedrooms sqft_basement</th>\n",
       "      <th>bedrooms sqft_living15</th>\n",
       "      <th>bedrooms sqft_lot15</th>\n",
       "      <th>bedrooms yr_sold</th>\n",
       "      <th>bedrooms yrs_old</th>\n",
       "      <th>bedrooms yrs_since_update</th>\n",
       "      <th>bathrooms^2</th>\n",
       "      <th>bathrooms sqft_living</th>\n",
       "      <th>bathrooms sqft_lot</th>\n",
       "      <th>bathrooms floors</th>\n",
       "      <th>bathrooms waterfront</th>\n",
       "      <th>bathrooms view</th>\n",
       "      <th>bathrooms condition</th>\n",
       "      <th>bathrooms grade</th>\n",
       "      <th>bathrooms sqft_above</th>\n",
       "      <th>bathrooms sqft_basement</th>\n",
       "      <th>bathrooms sqft_living15</th>\n",
       "      <th>bathrooms sqft_lot15</th>\n",
       "      <th>bathrooms yr_sold</th>\n",
       "      <th>bathrooms yrs_old</th>\n",
       "      <th>bathrooms yrs_since_update</th>\n",
       "      <th>sqft_living^2</th>\n",
       "      <th>sqft_living sqft_lot</th>\n",
       "      <th>sqft_living floors</th>\n",
       "      <th>...</th>\n",
       "      <th>98032</th>\n",
       "      <th>98033</th>\n",
       "      <th>98034</th>\n",
       "      <th>98038</th>\n",
       "      <th>98039</th>\n",
       "      <th>98040</th>\n",
       "      <th>98042</th>\n",
       "      <th>98045</th>\n",
       "      <th>98052</th>\n",
       "      <th>98053</th>\n",
       "      <th>98055</th>\n",
       "      <th>98056</th>\n",
       "      <th>98058</th>\n",
       "      <th>98059</th>\n",
       "      <th>98065</th>\n",
       "      <th>98070</th>\n",
       "      <th>98072</th>\n",
       "      <th>98074</th>\n",
       "      <th>98075</th>\n",
       "      <th>98077</th>\n",
       "      <th>98092</th>\n",
       "      <th>98102</th>\n",
       "      <th>98103</th>\n",
       "      <th>98105</th>\n",
       "      <th>98106</th>\n",
       "      <th>98107</th>\n",
       "      <th>98108</th>\n",
       "      <th>98109</th>\n",
       "      <th>98112</th>\n",
       "      <th>98115</th>\n",
       "      <th>98116</th>\n",
       "      <th>98117</th>\n",
       "      <th>98118</th>\n",
       "      <th>98119</th>\n",
       "      <th>98122</th>\n",
       "      <th>98125</th>\n",
       "      <th>98126</th>\n",
       "      <th>98133</th>\n",
       "      <th>98136</th>\n",
       "      <th>98144</th>\n",
       "      <th>98146</th>\n",
       "      <th>98148</th>\n",
       "      <th>98155</th>\n",
       "      <th>98166</th>\n",
       "      <th>98168</th>\n",
       "      <th>98177</th>\n",
       "      <th>98178</th>\n",
       "      <th>98188</th>\n",
       "      <th>98198</th>\n",
       "      <th>98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.698568</td>\n",
       "      <td>0.176627</td>\n",
       "      <td>-0.312662</td>\n",
       "      <td>-0.045650</td>\n",
       "      <td>-0.913867</td>\n",
       "      <td>-0.078021</td>\n",
       "      <td>-0.303802</td>\n",
       "      <td>-0.628011</td>\n",
       "      <td>-0.563268</td>\n",
       "      <td>-0.686584</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>-0.356763</td>\n",
       "      <td>-0.139512</td>\n",
       "      <td>1.446652</td>\n",
       "      <td>-0.270957</td>\n",
       "      <td>-0.208109</td>\n",
       "      <td>0.572810</td>\n",
       "      <td>0.361423</td>\n",
       "      <td>-0.066559</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>-0.457905</td>\n",
       "      <td>-0.073901</td>\n",
       "      <td>-0.29044</td>\n",
       "      <td>0.128589</td>\n",
       "      <td>0.195405</td>\n",
       "      <td>-0.361020</td>\n",
       "      <td>0.647588</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>-0.082655</td>\n",
       "      <td>0.700065</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.070817</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>-0.219357</td>\n",
       "      <td>-0.043495</td>\n",
       "      <td>-0.538131</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.278164</td>\n",
       "      <td>-0.141553</td>\n",
       "      <td>-0.127092</td>\n",
       "      <td>-0.422112</td>\n",
       "      <td>0.457068</td>\n",
       "      <td>-0.196322</td>\n",
       "      <td>-0.110202</td>\n",
       "      <td>0.177620</td>\n",
       "      <td>-0.005760</td>\n",
       "      <td>0.077292</td>\n",
       "      <td>-0.361541</td>\n",
       "      <td>-0.107329</td>\n",
       "      <td>-0.674778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.142645</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>-0.168078</td>\n",
       "      <td>-0.048494</td>\n",
       "      <td>-0.115712</td>\n",
       "      <td>-0.161154</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.166323</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>6.870615</td>\n",
       "      <td>-0.151198</td>\n",
       "      <td>-0.118956</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.11266</td>\n",
       "      <td>-0.141515</td>\n",
       "      <td>-0.133832</td>\n",
       "      <td>-0.094712</td>\n",
       "      <td>-0.127227</td>\n",
       "      <td>-0.072202</td>\n",
       "      <td>-0.171351</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.112941</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.069113</td>\n",
       "      <td>-0.111814</td>\n",
       "      <td>-0.169047</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.165734</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.118689</td>\n",
       "      <td>-0.139228</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.154581</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.131179</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>-0.147307</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.1135</td>\n",
       "      <td>-0.108075</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.123673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.698568</td>\n",
       "      <td>0.502201</td>\n",
       "      <td>0.245442</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.930263</td>\n",
       "      <td>-0.078021</td>\n",
       "      <td>-0.303802</td>\n",
       "      <td>0.912485</td>\n",
       "      <td>-0.563268</td>\n",
       "      <td>0.623003</td>\n",
       "      <td>-0.647824</td>\n",
       "      <td>-0.239606</td>\n",
       "      <td>-0.095542</td>\n",
       "      <td>-0.691251</td>\n",
       "      <td>0.170889</td>\n",
       "      <td>0.240206</td>\n",
       "      <td>0.572810</td>\n",
       "      <td>0.600686</td>\n",
       "      <td>0.351931</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>1.164248</td>\n",
       "      <td>-0.073901</td>\n",
       "      <td>-0.29044</td>\n",
       "      <td>1.161826</td>\n",
       "      <td>0.195405</td>\n",
       "      <td>0.674970</td>\n",
       "      <td>-0.578532</td>\n",
       "      <td>0.095976</td>\n",
       "      <td>-0.032463</td>\n",
       "      <td>0.697861</td>\n",
       "      <td>0.498992</td>\n",
       "      <td>0.577371</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>0.205261</td>\n",
       "      <td>0.031315</td>\n",
       "      <td>0.781819</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.278164</td>\n",
       "      <td>1.031659</td>\n",
       "      <td>0.086992</td>\n",
       "      <td>0.444244</td>\n",
       "      <td>-0.537788</td>\n",
       "      <td>0.020671</td>\n",
       "      <td>-0.042172</td>\n",
       "      <td>0.501693</td>\n",
       "      <td>0.711020</td>\n",
       "      <td>0.848808</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.021193</td>\n",
       "      <td>0.600008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.142645</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>-0.168078</td>\n",
       "      <td>-0.048494</td>\n",
       "      <td>-0.115712</td>\n",
       "      <td>6.205230</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.166323</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>-0.145547</td>\n",
       "      <td>-0.151198</td>\n",
       "      <td>-0.118956</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.11266</td>\n",
       "      <td>-0.141515</td>\n",
       "      <td>-0.133832</td>\n",
       "      <td>-0.094712</td>\n",
       "      <td>-0.127227</td>\n",
       "      <td>-0.072202</td>\n",
       "      <td>-0.171351</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.112941</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.069113</td>\n",
       "      <td>-0.111814</td>\n",
       "      <td>-0.169047</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.165734</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.118689</td>\n",
       "      <td>-0.139228</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.154581</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.131179</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>-0.147307</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.1135</td>\n",
       "      <td>-0.108075</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.123673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.808398</td>\n",
       "      <td>1.478923</td>\n",
       "      <td>0.989580</td>\n",
       "      <td>-0.193725</td>\n",
       "      <td>1.852328</td>\n",
       "      <td>-0.078021</td>\n",
       "      <td>-0.303802</td>\n",
       "      <td>-0.628011</td>\n",
       "      <td>1.146006</td>\n",
       "      <td>1.447558</td>\n",
       "      <td>-0.647824</td>\n",
       "      <td>-0.459276</td>\n",
       "      <td>-0.207244</td>\n",
       "      <td>1.446652</td>\n",
       "      <td>-0.916731</td>\n",
       "      <td>-1.173709</td>\n",
       "      <td>1.919739</td>\n",
       "      <td>2.096077</td>\n",
       "      <td>1.521244</td>\n",
       "      <td>-0.112630</td>\n",
       "      <td>2.989170</td>\n",
       "      <td>-0.073901</td>\n",
       "      <td>-0.29044</td>\n",
       "      <td>0.903517</td>\n",
       "      <td>2.041759</td>\n",
       "      <td>2.041901</td>\n",
       "      <td>-0.578532</td>\n",
       "      <td>0.392149</td>\n",
       "      <td>-0.083723</td>\n",
       "      <td>1.810273</td>\n",
       "      <td>-0.544770</td>\n",
       "      <td>-0.932552</td>\n",
       "      <td>1.493161</td>\n",
       "      <td>1.175287</td>\n",
       "      <td>-0.103571</td>\n",
       "      <td>2.281763</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.278164</td>\n",
       "      <td>0.941412</td>\n",
       "      <td>1.524415</td>\n",
       "      <td>1.578723</td>\n",
       "      <td>-0.537788</td>\n",
       "      <td>0.320531</td>\n",
       "      <td>-0.069877</td>\n",
       "      <td>1.480377</td>\n",
       "      <td>-0.430678</td>\n",
       "      <td>-0.920598</td>\n",
       "      <td>0.696558</td>\n",
       "      <td>-0.126264</td>\n",
       "      <td>1.892940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.142645</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>-0.168078</td>\n",
       "      <td>-0.048494</td>\n",
       "      <td>-0.115712</td>\n",
       "      <td>-0.161154</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.166323</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>-0.145547</td>\n",
       "      <td>-0.151198</td>\n",
       "      <td>-0.118956</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.11266</td>\n",
       "      <td>-0.141515</td>\n",
       "      <td>-0.133832</td>\n",
       "      <td>-0.094712</td>\n",
       "      <td>-0.127227</td>\n",
       "      <td>-0.072202</td>\n",
       "      <td>-0.171351</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.112941</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.069113</td>\n",
       "      <td>-0.111814</td>\n",
       "      <td>5.915516</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.165734</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.118689</td>\n",
       "      <td>-0.139228</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.154581</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.131179</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>-0.147307</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.1135</td>\n",
       "      <td>-0.108075</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.123673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.411261</td>\n",
       "      <td>-1.451244</td>\n",
       "      <td>-1.067744</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>-0.913867</td>\n",
       "      <td>-0.078021</td>\n",
       "      <td>-0.303802</td>\n",
       "      <td>-0.628011</td>\n",
       "      <td>-0.563268</td>\n",
       "      <td>-0.832094</td>\n",
       "      <td>-0.647824</td>\n",
       "      <td>-0.664300</td>\n",
       "      <td>0.225118</td>\n",
       "      <td>-0.691251</td>\n",
       "      <td>-0.236968</td>\n",
       "      <td>-0.173623</td>\n",
       "      <td>-0.474802</td>\n",
       "      <td>-1.074152</td>\n",
       "      <td>-0.858410</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.863443</td>\n",
       "      <td>-0.073901</td>\n",
       "      <td>-0.29044</td>\n",
       "      <td>-0.646340</td>\n",
       "      <td>-0.564858</td>\n",
       "      <td>-0.739924</td>\n",
       "      <td>-0.578532</td>\n",
       "      <td>-0.648609</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>-0.411796</td>\n",
       "      <td>-0.317040</td>\n",
       "      <td>-0.260392</td>\n",
       "      <td>-1.101820</td>\n",
       "      <td>-0.941021</td>\n",
       "      <td>-0.155357</td>\n",
       "      <td>-1.138109</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.278164</td>\n",
       "      <td>-1.495258</td>\n",
       "      <td>-1.197513</td>\n",
       "      <td>-0.896373</td>\n",
       "      <td>-0.537788</td>\n",
       "      <td>-1.011327</td>\n",
       "      <td>-0.129178</td>\n",
       "      <td>-1.451473</td>\n",
       "      <td>-0.782631</td>\n",
       "      <td>-0.758903</td>\n",
       "      <td>-0.733254</td>\n",
       "      <td>-0.142080</td>\n",
       "      <td>-0.987804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.142645</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>-0.168078</td>\n",
       "      <td>-0.048494</td>\n",
       "      <td>-0.115712</td>\n",
       "      <td>-0.161154</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.166323</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>-0.145547</td>\n",
       "      <td>-0.151198</td>\n",
       "      <td>-0.118956</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.11266</td>\n",
       "      <td>-0.141515</td>\n",
       "      <td>-0.133832</td>\n",
       "      <td>-0.094712</td>\n",
       "      <td>-0.127227</td>\n",
       "      <td>-0.072202</td>\n",
       "      <td>-0.171351</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.112941</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.069113</td>\n",
       "      <td>-0.111814</td>\n",
       "      <td>-0.169047</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.165734</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.118689</td>\n",
       "      <td>-0.139228</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.154581</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.131179</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>-0.147307</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.1135</td>\n",
       "      <td>-0.108075</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.123673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.411261</td>\n",
       "      <td>-1.451244</td>\n",
       "      <td>-0.980198</td>\n",
       "      <td>-0.162072</td>\n",
       "      <td>-0.913867</td>\n",
       "      <td>-0.078021</td>\n",
       "      <td>-0.303802</td>\n",
       "      <td>-0.628011</td>\n",
       "      <td>-0.563268</td>\n",
       "      <td>-0.735088</td>\n",
       "      <td>-0.647824</td>\n",
       "      <td>-0.356763</td>\n",
       "      <td>-0.211734</td>\n",
       "      <td>1.446652</td>\n",
       "      <td>0.680711</td>\n",
       "      <td>0.757492</td>\n",
       "      <td>-0.474802</td>\n",
       "      <td>-1.074152</td>\n",
       "      <td>-0.809175</td>\n",
       "      <td>-0.181779</td>\n",
       "      <td>-0.863443</td>\n",
       "      <td>-0.073901</td>\n",
       "      <td>-0.29044</td>\n",
       "      <td>-0.646340</td>\n",
       "      <td>-0.564858</td>\n",
       "      <td>-0.682369</td>\n",
       "      <td>-0.578532</td>\n",
       "      <td>-0.474226</td>\n",
       "      <td>-0.240066</td>\n",
       "      <td>-0.410144</td>\n",
       "      <td>0.451548</td>\n",
       "      <td>0.528664</td>\n",
       "      <td>-1.101820</td>\n",
       "      <td>-0.921300</td>\n",
       "      <td>-0.236174</td>\n",
       "      <td>-1.138109</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.278164</td>\n",
       "      <td>-1.495258</td>\n",
       "      <td>-1.197513</td>\n",
       "      <td>-0.873309</td>\n",
       "      <td>-0.537788</td>\n",
       "      <td>-0.939565</td>\n",
       "      <td>-0.282643</td>\n",
       "      <td>-1.450827</td>\n",
       "      <td>-0.319084</td>\n",
       "      <td>-0.259958</td>\n",
       "      <td>-0.699253</td>\n",
       "      <td>-0.211841</td>\n",
       "      <td>-0.951511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.142645</td>\n",
       "      <td>-0.160346</td>\n",
       "      <td>-0.168078</td>\n",
       "      <td>-0.048494</td>\n",
       "      <td>-0.115712</td>\n",
       "      <td>-0.161154</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.166323</td>\n",
       "      <td>-0.13714</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>-0.145547</td>\n",
       "      <td>-0.151198</td>\n",
       "      <td>-0.118956</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.11266</td>\n",
       "      <td>-0.141515</td>\n",
       "      <td>-0.133832</td>\n",
       "      <td>-0.094712</td>\n",
       "      <td>-0.127227</td>\n",
       "      <td>-0.072202</td>\n",
       "      <td>-0.171351</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.112941</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.069113</td>\n",
       "      <td>-0.111814</td>\n",
       "      <td>-0.169047</td>\n",
       "      <td>-0.123157</td>\n",
       "      <td>-0.165734</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.094046</td>\n",
       "      <td>-0.118689</td>\n",
       "      <td>7.182455</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-0.154581</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.131179</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>-0.147307</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.1135</td>\n",
       "      <td>-0.108075</td>\n",
       "      <td>-0.107782</td>\n",
       "      <td>-0.076811</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>-0.123673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront      view  \\\n",
       "0  0.698568   0.176627    -0.312662 -0.045650 -0.913867   -0.078021 -0.303802   \n",
       "1  0.698568   0.502201     0.245442  0.002568  0.930263   -0.078021 -0.303802   \n",
       "2  1.808398   1.478923     0.989580 -0.193725  1.852328   -0.078021 -0.303802   \n",
       "3 -0.411261  -1.451244    -1.067744  0.054619 -0.913867   -0.078021 -0.303802   \n",
       "4 -0.411261  -1.451244    -0.980198 -0.162072 -0.913867   -0.078021 -0.303802   \n",
       "\n",
       "   condition     grade  sqft_above  sqft_basement  sqft_living15  sqft_lot15  \\\n",
       "0  -0.628011 -0.563268   -0.686584       0.654619      -0.356763   -0.139512   \n",
       "1   0.912485 -0.563268    0.623003      -0.647824      -0.239606   -0.095542   \n",
       "2  -0.628011  1.146006    1.447558      -0.647824      -0.459276   -0.207244   \n",
       "3  -0.628011 -0.563268   -0.832094      -0.647824      -0.664300    0.225118   \n",
       "4  -0.628011 -0.563268   -0.735088      -0.647824      -0.356763   -0.211734   \n",
       "\n",
       "    yr_sold   yrs_old  yrs_since_update  bedrooms^2  bedrooms bathrooms  \\\n",
       "0  1.446652 -0.270957         -0.208109    0.572810            0.361423   \n",
       "1 -0.691251  0.170889          0.240206    0.572810            0.600686   \n",
       "2  1.446652 -0.916731         -1.173709    1.919739            2.096077   \n",
       "3 -0.691251 -0.236968         -0.173623   -0.474802           -1.074152   \n",
       "4  1.446652  0.680711          0.757492   -0.474802           -1.074152   \n",
       "\n",
       "   bedrooms sqft_living  bedrooms sqft_lot  bedrooms floors  \\\n",
       "0             -0.066559           0.003965        -0.457905   \n",
       "1              0.351931           0.057481         1.164248   \n",
       "2              1.521244          -0.112630         2.989170   \n",
       "3             -0.858410          -0.001406        -0.863443   \n",
       "4             -0.809175          -0.181779        -0.863443   \n",
       "\n",
       "   bedrooms waterfront  bedrooms view  bedrooms condition  bedrooms grade  \\\n",
       "0            -0.073901       -0.29044            0.128589        0.195405   \n",
       "1            -0.073901       -0.29044            1.161826        0.195405   \n",
       "2            -0.073901       -0.29044            0.903517        2.041759   \n",
       "3            -0.073901       -0.29044           -0.646340       -0.564858   \n",
       "4            -0.073901       -0.29044           -0.646340       -0.564858   \n",
       "\n",
       "   bedrooms sqft_above  bedrooms sqft_basement  bedrooms sqft_living15  \\\n",
       "0            -0.361020                0.647588                0.007401   \n",
       "1             0.674970               -0.578532                0.095976   \n",
       "2             2.041901               -0.578532                0.392149   \n",
       "3            -0.739924               -0.578532               -0.648609   \n",
       "4            -0.682369               -0.578532               -0.474226   \n",
       "\n",
       "   bedrooms sqft_lot15  bedrooms yr_sold  bedrooms yrs_old  \\\n",
       "0            -0.082655          0.700065          0.005577   \n",
       "1            -0.032463          0.697861          0.498992   \n",
       "2            -0.083723          1.810273         -0.544770   \n",
       "3             0.133940         -0.411796         -0.317040   \n",
       "4            -0.240066         -0.410144          0.451548   \n",
       "\n",
       "   bedrooms yrs_since_update  bathrooms^2  bathrooms sqft_living  \\\n",
       "0                   0.070817     0.000623              -0.219357   \n",
       "1                   0.577371     0.322876               0.205261   \n",
       "2                  -0.932552     1.493161               1.175287   \n",
       "3                  -0.260392    -1.101820              -0.941021   \n",
       "4                   0.528664    -1.101820              -0.921300   \n",
       "\n",
       "   bathrooms sqft_lot  bathrooms floors  bathrooms waterfront  bathrooms view  \\\n",
       "0           -0.043495         -0.538131             -0.072187       -0.278164   \n",
       "1            0.031315          0.781819             -0.072187       -0.278164   \n",
       "2           -0.103571          2.281763             -0.072187       -0.278164   \n",
       "3           -0.155357         -1.138109             -0.072187       -0.278164   \n",
       "4           -0.236174         -1.138109             -0.072187       -0.278164   \n",
       "\n",
       "   bathrooms condition  bathrooms grade  bathrooms sqft_above  \\\n",
       "0            -0.141553        -0.127092             -0.422112   \n",
       "1             1.031659         0.086992              0.444244   \n",
       "2             0.941412         1.524415              1.578723   \n",
       "3            -1.495258        -1.197513             -0.896373   \n",
       "4            -1.495258        -1.197513             -0.873309   \n",
       "\n",
       "   bathrooms sqft_basement  bathrooms sqft_living15  bathrooms sqft_lot15  \\\n",
       "0                 0.457068                -0.196322             -0.110202   \n",
       "1                -0.537788                 0.020671             -0.042172   \n",
       "2                -0.537788                 0.320531             -0.069877   \n",
       "3                -0.537788                -1.011327             -0.129178   \n",
       "4                -0.537788                -0.939565             -0.282643   \n",
       "\n",
       "   bathrooms yr_sold  bathrooms yrs_old  bathrooms yrs_since_update  \\\n",
       "0           0.177620          -0.005760                    0.077292   \n",
       "1           0.501693           0.711020                    0.848808   \n",
       "2           1.480377          -0.430678                   -0.920598   \n",
       "3          -1.451473          -0.782631                   -0.758903   \n",
       "4          -1.450827          -0.319084                   -0.259958   \n",
       "\n",
       "   sqft_living^2  sqft_living sqft_lot  sqft_living floors  ...     98032  \\\n",
       "0      -0.361541             -0.107329           -0.674778  ... -0.076811   \n",
       "1       0.027284             -0.021193            0.600008  ... -0.076811   \n",
       "2       0.696558             -0.126264            1.892940  ... -0.076811   \n",
       "3      -0.733254             -0.142080           -0.987804  ... -0.076811   \n",
       "4      -0.699253             -0.211841           -0.951511  ... -0.076811   \n",
       "\n",
       "      98033     98034     98038     98039     98040     98042     98045  \\\n",
       "0 -0.142645 -0.160346 -0.168078 -0.048494 -0.115712 -0.161154 -0.098296   \n",
       "1 -0.142645 -0.160346 -0.168078 -0.048494 -0.115712  6.205230 -0.098296   \n",
       "2 -0.142645 -0.160346 -0.168078 -0.048494 -0.115712 -0.161154 -0.098296   \n",
       "3 -0.142645 -0.160346 -0.168078 -0.048494 -0.115712 -0.161154 -0.098296   \n",
       "4 -0.142645 -0.160346 -0.168078 -0.048494 -0.115712 -0.161154 -0.098296   \n",
       "\n",
       "      98052    98053     98055     98056     98058     98059     98065  \\\n",
       "0 -0.166323 -0.13714 -0.108658 -0.135967  6.870615 -0.151198 -0.118956   \n",
       "1 -0.166323 -0.13714 -0.108658 -0.135967 -0.145547 -0.151198 -0.118956   \n",
       "2 -0.166323 -0.13714 -0.108658 -0.135967 -0.145547 -0.151198 -0.118956   \n",
       "3 -0.166323 -0.13714 -0.108658 -0.135967 -0.145547 -0.151198 -0.118956   \n",
       "4 -0.166323 -0.13714 -0.108658 -0.135967 -0.145547 -0.151198 -0.118956   \n",
       "\n",
       "      98070    98072     98074     98075     98077     98092     98102  \\\n",
       "0 -0.075167 -0.11266 -0.141515 -0.133832 -0.094712 -0.127227 -0.072202   \n",
       "1 -0.075167 -0.11266 -0.141515 -0.133832 -0.094712 -0.127227 -0.072202   \n",
       "2 -0.075167 -0.11266 -0.141515 -0.133832 -0.094712 -0.127227 -0.072202   \n",
       "3 -0.075167 -0.11266 -0.141515 -0.133832 -0.094712 -0.127227 -0.072202   \n",
       "4 -0.075167 -0.11266 -0.141515 -0.133832 -0.094712 -0.127227 -0.072202   \n",
       "\n",
       "      98103     98105     98106     98107     98108     98109     98112  \\\n",
       "0 -0.171351 -0.102068 -0.123157 -0.112941 -0.094046 -0.069113 -0.111814   \n",
       "1 -0.171351 -0.102068 -0.123157 -0.112941 -0.094046 -0.069113 -0.111814   \n",
       "2 -0.171351 -0.102068 -0.123157 -0.112941 -0.094046 -0.069113 -0.111814   \n",
       "3 -0.171351 -0.102068 -0.123157 -0.112941 -0.094046 -0.069113 -0.111814   \n",
       "4 -0.171351 -0.102068 -0.123157 -0.112941 -0.094046 -0.069113 -0.111814   \n",
       "\n",
       "      98115     98116     98117     98118     98119     98122     98125  \\\n",
       "0 -0.169047 -0.123157 -0.165734 -0.153531 -0.094046 -0.118689 -0.139228   \n",
       "1 -0.169047 -0.123157 -0.165734 -0.153531 -0.094046 -0.118689 -0.139228   \n",
       "2  5.915516 -0.123157 -0.165734 -0.153531 -0.094046 -0.118689 -0.139228   \n",
       "3 -0.169047 -0.123157 -0.165734 -0.153531 -0.094046 -0.118689 -0.139228   \n",
       "4 -0.169047 -0.123157 -0.165734 -0.153531 -0.094046 -0.118689  7.182455   \n",
       "\n",
       "      98126     98133     98136     98144     98146     98148     98155  \\\n",
       "0 -0.126976 -0.154581 -0.112379 -0.131179 -0.118153 -0.051593 -0.147307   \n",
       "1 -0.126976 -0.154581 -0.112379 -0.131179 -0.118153 -0.051593 -0.147307   \n",
       "2 -0.126976 -0.154581 -0.112379 -0.131179 -0.118153 -0.051593 -0.147307   \n",
       "3 -0.126976 -0.154581 -0.112379 -0.131179 -0.118153 -0.051593 -0.147307   \n",
       "4 -0.126976 -0.154581 -0.112379 -0.131179 -0.118153 -0.051593 -0.147307   \n",
       "\n",
       "      98166   98168     98177     98178     98188     98198     98199  \n",
       "0 -0.107782 -0.1135 -0.108075 -0.107782 -0.076811 -0.112379 -0.123673  \n",
       "1 -0.107782 -0.1135 -0.108075 -0.107782 -0.076811 -0.112379 -0.123673  \n",
       "2 -0.107782 -0.1135 -0.108075 -0.107782 -0.076811 -0.112379 -0.123673  \n",
       "3 -0.107782 -0.1135 -0.108075 -0.107782 -0.076811 -0.112379 -0.123673  \n",
       "4 -0.107782 -0.1135 -0.108075 -0.107782 -0.076811 -0.112379 -0.123673  \n",
       "\n",
       "[5 rows x 221 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit A linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539494.636908079\n",
      "[ 1.13793508e+07 -7.41928182e+05  6.22339391e+07 -2.29188888e+06\n",
      " -8.24961903e+06 -1.53669862e+07 -4.03607873e+06  9.19128116e+06\n",
      " -2.08747356e+07 -7.11163789e+07 -4.90075231e+07 -7.32850472e+05\n",
      " -6.15071294e+06 -1.08388902e+04 -2.59300526e+07  6.61301427e+05\n",
      "  1.96501248e+04 -6.31979763e+03 -6.49225001e+04  3.57506428e+04\n",
      "  4.43281688e+04 -8.89753009e+03 -2.06618551e+03 -1.09557563e+02\n",
      "  3.28850009e+04 -9.33092207e+04 -2.63402540e+04  9.05429802e+03\n",
      " -2.01521186e+04 -1.13843338e+07 -3.32848757e+04  3.23352446e+04\n",
      " -6.70011907e+04  1.73212408e+05 -1.24505074e+04 -2.58125792e+04\n",
      "  2.62001921e+04 -1.84026037e+04 -1.84196791e+04  4.54463961e+04\n",
      " -1.03645133e+04 -1.88194686e+04  1.16023638e+04 -2.15942307e+03\n",
      "  7.67545773e+05 -3.98545880e+04  1.86555253e+04 -2.27441325e+05\n",
      " -6.65397042e+04 -2.79136762e+05  4.01344018e+04 -9.18158499e+04\n",
      " -2.11668860e+04  1.85589373e+05  2.56384648e+05  2.64245812e+04\n",
      "  1.52543267e+05  2.72548762e+04 -6.21852461e+07  2.21372204e+05\n",
      " -1.92033304e+05 -1.05230126e+04  1.90824565e+04  9.55675140e+02\n",
      " -1.01511398e+04  2.05616138e+03  5.22277765e+04 -1.94808931e+04\n",
      " -5.08720708e+03  2.05616501e+04  8.69713535e+03  2.28198715e+06\n",
      " -9.14257775e+03 -3.89872170e+03  2.68888081e+04 -3.91437637e+04\n",
      "  1.47232622e+04  3.15682675e+04 -3.75384090e+04  2.85950378e+05\n",
      "  6.27749969e+04  3.27229107e+04 -1.12666732e+04  8.13990061e+06\n",
      "  1.64399696e+04  1.06243753e+04 -1.53669862e+07 -1.76359667e+04\n",
      "  8.25783282e+03 -7.24377002e+04  4.32380156e+04 -1.04180439e+04\n",
      "  2.61573013e+04 -3.15104044e+03  3.08060659e+07 -6.95568249e+03\n",
      " -1.15236729e+04  2.45307735e+04  1.70183539e+04  1.23382586e+05\n",
      "  4.49754368e+04  2.71114506e+04  1.79706377e+04 -6.38941527e+02\n",
      "  3.91963976e+06 -6.05415135e+03  1.43398033e+04 -1.53726676e+02\n",
      " -1.73293589e+04  4.46681929e+04  4.73591076e+04  4.27782923e+04\n",
      " -1.38959898e+03 -9.20821726e+06  2.01756620e+04  1.44333747e+04\n",
      "  1.63713273e+05  1.13706807e+05  5.85620819e+04 -4.59670812e+04\n",
      " -3.53486811e+04  2.06769958e+07  1.40049168e+05 -4.36930490e+04\n",
      " -6.00173338e+04  4.30849485e+04 -1.54293478e+05 -2.66099813e+03\n",
      "  7.09760857e+07 -9.32705031e+04  8.98314308e+04 -3.20948360e+03\n",
      " -4.39848591e+04 -3.50476886e+03  4.88829277e+07 -3.75079011e+04\n",
      "  3.96726282e+04  2.53604711e+04 -2.44039972e+04  6.72082629e+05\n",
      "  7.36966301e+04 -3.10498125e+04  4.02494397e+03  6.21683763e+06\n",
      " -7.01047203e+03  2.60151102e+03 -1.08388907e+04  2.57046681e+07\n",
      " -7.01824222e+05 -4.19219518e+04  3.90194916e+04  6.67531604e+04\n",
      "  1.26434200e+03  1.19203170e+02  8.82470451e+04  3.00157319e+04\n",
      "  3.97692290e+04  2.14047091e+04  3.19183151e+04  4.40816680e+03\n",
      "  1.46788874e+04  6.01628151e+03  7.38765762e+03 -2.30547802e+02\n",
      " -3.69128175e+03  1.10982856e+04  2.49596567e+04  1.55774398e+04\n",
      "  2.86625675e+04  3.95741295e+02  1.68375705e+03  1.04002902e+03\n",
      "  5.19388721e+04  3.27539155e+04  4.75094783e+03  5.30028154e+04\n",
      "  5.38327481e+04 -8.12278695e+02  1.02779317e+04  4.20457063e+04\n",
      "  2.69778763e+04  5.40459973e+03  1.39402229e+04  5.86304365e+03\n",
      "  1.35566192e+04  1.16945579e+04  5.04430958e+03  1.88010120e+04\n",
      "  2.89185444e+04  2.54667944e+04  1.37301333e+04 -3.71784178e+03\n",
      "  3.27608429e+04  6.14160555e+04  4.65282638e+04  1.66898963e+04\n",
      "  4.23336617e+04  1.27301318e+04  3.21412358e+04  6.37518134e+04\n",
      "  5.92629871e+04  3.82708519e+04  5.52649487e+04  2.61723991e+04\n",
      "  4.64353722e+04  4.12316339e+04  3.11817981e+04  2.65698060e+04\n",
      "  2.70651795e+04  3.02513935e+04  3.40208765e+04  1.32363804e+04\n",
      "  4.04669883e+03  2.31982944e+04  1.01601940e+04  7.59445230e+03\n",
      "  2.48447690e+04  7.35540449e+03  3.41070975e+03  3.51495352e+03\n",
      "  4.77688750e+04]\n"
     ]
    }
   ],
   "source": [
    "#instantiate a linear regression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "#fit the linear regression to the data\n",
    "lm = lm.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(lm.intercept_)\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train_scaled)\n",
    "y_pred = lm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 128139\n",
      "Test RMSE: 138409\n"
     ]
    }
   ],
   "source": [
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('Training RMSE:', int(train_rmse) )\n",
    "print('Test RMSE:',  int(test_rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Recursive Feature Elimiation to select certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "ols = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 209 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFECV(cv=5,\n",
       "      estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                 normalize=False),\n",
       "      min_features_to_select=1, n_jobs=-1, scoring='neg_mean_squared_error',\n",
       "      step=2, verbose=3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recursive wrapper method\n",
    "\n",
    "# Create recursive feature eliminator that scores features by mean squared errors\n",
    "selector = RFECV(estimator=ols, step=2, cv=5, scoring='neg_mean_squared_error', verbose =3, n_jobs=-1)\n",
    "\n",
    "# Fit recursive feature eliminator \n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_train_scaled.columns[selector.support_]\n",
    "removed_columns = X_train_scaled.columns[~selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit linear model with only selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 128163 Test RMSE:  138408\n"
     ]
    }
   ],
   "source": [
    "lm_rfe = LinearRegression()\n",
    "\n",
    "lm_rfe = lm_rfe.fit(X_train_scaled[selected_columns], y_train)\n",
    "\n",
    "y_rfe=lm_rfe.predict(X_train_scaled[selected_columns])\n",
    "\n",
    "trainRFE_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_rfe))\n",
    "\n",
    "\n",
    "y_pred_rfe = lm_rfe.predict(X_test_scaled[selected_columns])\n",
    "\n",
    "testRFE_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred_rfe))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train RMSE\", int(trainRFE_rmse), \"Test RMSE: \", int(testRFE_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Lasso model with a alpha of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 77996 Test RMSE:  137318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 134351565176355.19, tolerance: 208470859275.807\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01, normalize=False)\n",
    "\n",
    "lasso.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_train_pred_lasso = lasso.predict(X_train_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "train_rmse_lasso = metrics.mean_absolute_error(y_train, y_train_pred_lasso)\n",
    "test_rmse_lasso = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso))\n",
    "\n",
    "\n",
    "print(\"Train RMSE\", int(train_rmse_lasso), \"Test RMSE: \", int(test_rmse_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.32082628e+04,  1.01202725e+05,  2.03469429e+04,  2.06904903e+04,\n",
       "       -3.62491076e+04,  1.09931341e+05,  4.65238323e+04,  1.21071154e+04,\n",
       "        2.90312748e+04,  2.71232653e+04,  2.25019020e+04, -1.89764276e+04,\n",
       "        4.22777795e+03,  1.15142977e+04, -5.13655007e+04, -2.37858769e+04,\n",
       "        1.94224927e+04, -4.08695445e+03, -1.32707245e+05,  3.36485256e+04,\n",
       "        4.27377765e+04, -1.01683415e+04, -3.82441830e+03, -8.88787770e+02,\n",
       "        3.52295281e+04, -3.29382209e+04, -2.73570358e+02,  7.47990564e+03,\n",
       "       -1.87978518e+04, -4.98633886e+04, -2.99598096e+04,  3.07463887e+04,\n",
       "       -7.01974768e+04,  1.28819447e+05, -9.18955435e+03, -2.78727002e+04,\n",
       "        3.42141036e+04, -2.12264924e+04, -1.67855031e+04,  6.83266096e+04,\n",
       "        2.86902876e+04, -3.69042896e+03,  2.03982488e+03, -1.57011622e+03,\n",
       "       -8.65544888e+04, -3.51369204e+04,  1.42418802e+04,  5.10556373e+04,\n",
       "       -3.72026310e+04, -7.33850807e+04, -1.28329707e+04,  7.21160247e+04,\n",
       "        2.12605570e+04,  2.21442098e+05, -1.62499060e+05,  7.27813399e+04,\n",
       "       -4.11881319e+04,  7.44439774e+03, -9.76486692e+04,  1.31582492e+05,\n",
       "       -6.79239911e+04, -1.09749119e+04,  1.79526013e+04,  1.68546267e+02,\n",
       "       -9.39734394e+03,  2.61841118e+02,  5.24855077e+04, -4.40728735e+04,\n",
       "       -1.09961550e+04,  1.93685672e+04,  1.01137480e+04, -3.13619220e+04,\n",
       "       -6.92269595e+03, -4.17074233e+03,  2.70668445e+04, -3.69951546e+04,\n",
       "        1.68380016e+04,  3.00436273e+04, -3.45753292e+04,  8.43996836e+04,\n",
       "        4.84131504e+03,  3.38999807e+04, -1.14640691e+04, -7.00220194e+04,\n",
       "        1.00123662e+04,  1.66843614e+04, -3.13491826e+04, -2.06551208e+04,\n",
       "        5.18650101e+03, -6.83622010e+04,  7.59538604e+04,  4.81072825e+03,\n",
       "        2.85707330e+04, -3.66390742e+03, -1.24016625e+04, -4.13199421e+03,\n",
       "       -7.96955294e+03,  2.70145389e+04,  1.56038314e+04,  1.26372015e+05,\n",
       "       -7.90646576e+04, -2.11331241e+04,  1.48644103e+04, -1.15175145e+03,\n",
       "       -1.64950065e+05, -7.34985343e+03,  1.63378089e+04,  2.83414319e+02,\n",
       "       -2.44435282e+04,  1.25954186e+04,  2.56774609e+04,  4.67504277e+04,\n",
       "       -2.69150919e+03, -2.66337643e+04,  2.19680510e+04,  1.24429604e+04,\n",
       "        1.71418851e+05,  7.18057568e+04,  3.94638517e+04, -6.16421858e+04,\n",
       "       -3.58239959e+04, -2.33150121e+05,  1.53153881e+05, -4.25901444e+04,\n",
       "        1.05702961e+05,  3.33721737e+02,  2.99429184e+04,  1.45376546e+04,\n",
       "       -4.80132577e+04, -2.42671527e+04, -1.03358824e+04, -5.45376894e+04,\n",
       "        2.08573459e+04,  1.03919339e+02, -8.19955155e+04, -5.01270112e+03,\n",
       "       -7.89443015e+03,  2.58339327e+04, -2.36323984e+04, -3.38892471e+04,\n",
       "        6.93402464e+04, -2.95814407e+04,  2.97544597e+03,  6.24102379e+04,\n",
       "       -7.49205883e+03,  3.57100576e+03,  4.54174195e+03, -1.90765229e+05,\n",
       "       -9.80263837e+03, -3.60278379e+04,  4.62695942e+04,  5.39486862e+04,\n",
       "        1.18447758e+03,  1.68539112e+02,  8.84390084e+04,  2.99605454e+04,\n",
       "        3.99270710e+04,  2.13416717e+04,  3.20082884e+04,  4.31219237e+03,\n",
       "        1.45818683e+04,  5.83385425e+03,  7.20682580e+03, -4.06167714e+02,\n",
       "       -3.69449722e+03,  1.10775028e+04,  2.47640388e+04,  1.55561246e+04,\n",
       "        2.87332853e+04,  3.30614467e+02,  1.75041191e+03,  1.09244921e+03,\n",
       "        5.19158043e+04,  3.28440549e+04,  4.65379163e+03,  5.28421700e+04,\n",
       "        5.40290985e+04, -9.62379559e+02,  1.03401019e+04,  4.20672717e+04,\n",
       "        2.68995847e+04,  5.37022386e+03,  1.38071423e+04,  5.82144667e+03,\n",
       "        1.33969580e+04,  1.15927617e+04,  4.63415001e+03,  1.87109029e+04,\n",
       "        2.90770840e+04,  2.55996374e+04,  1.37847366e+04, -3.84337546e+03,\n",
       "        3.28271486e+04,  6.12205690e+04,  4.64905665e+04,  1.67214076e+04,\n",
       "        4.21853777e+04,  1.27622352e+04,  3.18779780e+04,  6.35587020e+04,\n",
       "        5.91000436e+04,  3.81779358e+04,  5.50301017e+04,  2.62214192e+04,\n",
       "        4.60126324e+04,  4.11774342e+04,  3.10734754e+04,  2.65609856e+04,\n",
       "        2.70127525e+04,  3.01369764e+04,  3.39296012e+04,  1.30607175e+04,\n",
       "        4.06111476e+03,  2.32497488e+04,  1.01819721e+04,  7.60123324e+03,\n",
       "        2.48176565e+04,  7.23729959e+03,  3.52784788e+03,  3.32284867e+03,\n",
       "        4.76553688e+04])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Ridge Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 77901 Test RMSE:  137267\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.01, normalize=False)\n",
    "\n",
    "ridge.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_train_pred_ridge = ridge.predict(X_train_scaled)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "train_rmse_ridge = metrics.mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "test_rmse_ridge = np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "\n",
    "print(\"Train RMSE\", int(train_rmse_ridge), \"Test RMSE: \", int(test_rmse_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pick our best model, but this is more complicated than just choosing between linear regression, Lasso, or Ridge. We now have to also consider the different models that we get from different alpha values for Ridge and Lasso.\n",
    "\n",
    "\n",
    "How do we determine the best model that will not overfit to the training data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src = \"./resources/train_test_valid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross-validation is a statistical method used to protect against overfitting a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for K-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Split the dataset into K **equal** partitions (or \"folds\").\n",
    "2. Use fold 1 as the **testing set** and the union of the other folds as the **training set**.\n",
    "3. Calculate **testing accuracy**.\n",
    "4. Repeat steps 2 and 3 K times, using a **different fold** as the testing set each time.\n",
    "5. Use the **average testing accuracy** as the estimate of out-of-sample accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram of **10-fold cross-validation:**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1354/1*qPMFLEbvc8QQf38Cf77wQg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate splitting a dataset of 25 observations into 5 folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False).split(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration                   Training set observations                   Testing set observations\n",
      "    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       \n",
      "    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       \n",
      "    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     \n",
      "    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     \n",
      "    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]     \n"
     ]
    }
   ],
   "source": [
    "# print the contents of each training and testing set\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Validation set observations'))\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset contains **25 observations** (numbered 0 through 24)\n",
    "- 5-fold cross-validation, thus it runs for **5 iterations**\n",
    "- For each iteration, every observation is either in the training set or the testing set, **but not both**\n",
    "- Every observation is in the testing set **exactly once**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing cross-validation to train/test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Advantages of **cross-validation:**\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "Advantages of **train/test split:**\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "- Simpler to examine the detailed results of the testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. K can be any number, but **K=10** is generally recommended\n",
    "2. For classification problems, **stratified sampling** is recommended for creating the folds\n",
    "    - Each response class should be represented with equal proportions in each of the K folds\n",
    "    - scikit-learn's `cross_val_score` function does this by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinging the right Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoCV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7113769426751.1875, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21934573825739.25, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11905633236193.219, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19363340635151.438, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6945402087038.1875, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      ".../anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6683417262625.344, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3845862120414.125, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6842985292442.75, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "../anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7774074518161.25, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6599165886632.625, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53357504402042.06, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 60651552362176.36, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52743441689592.36, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24973427294996.562, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58237054428013.66, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72552149736579.67, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77506838082643.53, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 71037442161379.88, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74746258362999.34, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31121444351227.516, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98483776696872.62, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99133150195498.84, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95927427418008.77, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98874372123875.17, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 75583240438427.78, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107669788231090.42, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105022560083269.55, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103482591816389.22, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100719950482775.03, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106288453205182.72, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108196923988432.61, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105384089717205.97, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103877498601984.28, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106715730449628.53, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102183971359311.94, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108113266720605.78, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105314776326467.14, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "../anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103676033303107.42, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106575982296573.66, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101770901484536.47, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108749223350721.72, tolerance: 168614673679.38058\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105855506561696.95, tolerance: 169642462901.23526\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104380965551446.16, tolerance: 164989899999.6022\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107206847476938.45, tolerance: 164305085505.24506\n",
      "  tol, rng, random, positive)\n",
      "./anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103829213780432.67, tolerance: 166327951071.0391\n",
      "  tol, rng, random, positive)\n",
      ".[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.8s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.9s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37712882398251.34, tolerance: 208470859275.807\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=[0.01, 0.05, 0.1, 0.05, 1, 5, 10, 50, 100], copy_X=True, cv=5,\n",
       "        eps=0.001, fit_intercept=True, max_iter=2000, n_alphas=100, n_jobs=-1,\n",
       "        normalize=False, positive=False, precompute='auto', random_state=0,\n",
       "        selection='cyclic', tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoCV_model = LassoCV(alphas = [0.01,0.05,0.1,0.05,1,5,10,50,100],cv=5, random_state=0, verbose=1, n_jobs=-1, max_iter=2000)\n",
    "lassoCV_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoCV_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+02, 5.e+01, 1.e+01, 5.e+00, 1.e+00, 1.e-01, 5.e-02, 5.e-02,\n",
       "       1.e-02])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoCV_model.alphas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 79068 Test RMSE:  137261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_pred_lassocv = lassoCV_model.predict(X_train_scaled)\n",
    "y_pred_lassocv = lassoCV_model.predict(X_test_scaled)\n",
    "\n",
    "train_rmse_lassocv = metrics.mean_absolute_error(y_train, y_train_pred_lassocv)\n",
    "test_rmse_lassocv = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lassocv))\n",
    "\n",
    "\n",
    "print(\"Train RMSE\", int(train_rmse_lassocv), \"Test RMSE: \", int(test_rmse_lassocv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RidgeCV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1.e-02, 1.e-01, 1.e+00, 5.e+00, 1.e+01, 5.e+01, 1.e+02]),\n",
       "        cv=5, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "        store_cv_values=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeCV_model = RidgeCV(alphas = [0.01,0.05,0.1,0.05,1,5,10,50,100], cv=5)\n",
    "RidgeCV_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeCV_model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have used cross validation to help us determine the best **alpha** for Ridge and Lasso, we can then use those fitted models to compare on our test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 77898 Test RMSE:  136918\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_pred_ridgecv = RidgeCV_model.predict(X_train_scaled)\n",
    "y_pred_ridgecv = RidgeCV_model.predict(X_test_scaled)\n",
    "\n",
    "train_rmse_ridgecv = metrics.mean_absolute_error(y_train, y_train_pred_ridgecv)\n",
    "test_rmse_ridgecv = np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridgecv))\n",
    "\n",
    "\n",
    "print(\"Train RMSE\", int(train_rmse_ridgecv), \"Test RMSE: \", int(test_rmse_ridgecv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements to cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeated cross-validation**\n",
    "\n",
    "- Repeat cross-validation multiple times (with **different random splits** of the data) and average the results\n",
    "- More reliable estimate of out-of-sample performance by **reducing the variance** associated with a single trial of cross-validation\n",
    "\n",
    "**Creating a hold-out set**\n",
    "\n",
    "- \"Hold out\" a portion of the data **before** beginning the model building process\n",
    "- Locate the best model using cross-validation on the remaining data, and test it **using the hold-out set**\n",
    "- More reliable estimate of out-of-sample performance since hold-out set is **truly out-of-sample**\n",
    "\n",
    "**Feature engineering and selection within cross-validation iterations**\n",
    "\n",
    "- Normally, feature engineering and selection occurs **before** cross-validation\n",
    "- Instead, perform all feature engineering and selection **within each cross-validation iteration**\n",
    "- More reliable estimate of out-of-sample performance since it **better mimics** the application of the model to out-of-sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learn documentation: [Cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html), [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- scikit-learn issue on GitHub: [MSE is negative when returned by cross_val_score](https://github.com/scikit-learn/scikit-learn/issues/2439)\n",
    "- Section 5.1 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) (11 pages) and related videos: [K-fold and leave-one-out cross-validation](https://www.youtube.com/watch?v=nZAM5OXrktY&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf) (14 minutes), [Cross-validation the right and wrong ways](https://www.youtube.com/watch?v=S06JpVoNaA0&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf) (10 minutes)\n",
    "- Scott Fortmann-Roe: [Accurately Measuring Model Prediction Error](http://scott.fortmann-roe.com/docs/MeasuringError.html)\n",
    "- Machine Learning Mastery: [An Introduction to Feature Selection](http://machinelearningmastery.com/an-introduction-to-feature-selection/)\n",
    "- Harvard CS109: [Cross-Validation: The Right and Wrong Way](https://github.com/cs109/content/blob/master/lec_10_cross_val.ipynb)\n",
    "- Journal of Cheminformatics: [Cross-validation pitfalls when selecting and assessing regression and classification models](http://www.jcheminf.com/content/pdf/1758-2946-6-10.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
